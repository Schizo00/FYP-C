{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naweenk/miniforge3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "dir_list = os.listdir('./Preprocessed')\n",
    "\n",
    "for i in tqdm(dir_list):\n",
    "    file_paths.append(f'./Preprocessed/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_length(file_path):\n",
    "    mean_list = []\n",
    "    for i in os.listdir(file_path):\n",
    "        if i == \"mesh.ply\" or i == \"Embeddings\":\n",
    "            continue\n",
    "        path = f\"{file_path}/{i}\"\n",
    "        signal, sr = torchaudio.load(path)\n",
    "        mean_list.append(signal.shape[1])\n",
    "\n",
    "    return mean(mean_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals = Parallel(n_jobs=-1, backend='loky')(delayed(signal_length)(file) for file in tqdm(file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_signal_len = mean(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_signal_len = int(mean_signal_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_signal_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_signals(target_len, signal):\n",
    "    if signal.shape[1] < target_len:\n",
    "        missing = target_len - signal.shape[1]\n",
    "        last_dim = (0, missing)\n",
    "        signal = torch.nn.functional.pad(signal, last_dim)\n",
    "\n",
    "        return signal\n",
    "\n",
    "    elif signal.shape[1] > target_len:\n",
    "        signal = signal[:, :target_len]\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    else:\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", run_opts={\"device\":\"mps\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(path):\n",
    "    for i in os.listdir(path):\n",
    "        if i == \"mesh.ply\" or i == \"Embeddings\":\n",
    "            continue\n",
    "        signal, fs = torchaudio.load(f\"{path}/{i}\")\n",
    "        signal = scale_signals(683433, signal)\n",
    "        # mfccs = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=15)\n",
    "        # delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        # delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "        # embeddings = np.concatenate((mfccs, delta_mfccs, delta2_mfccs)).flatten()\n",
    "        # print(embeddings.shape)\n",
    "\n",
    "        embeddings = classifier.encode_batch(signal)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        try:\n",
    "            os.mkdir(f\"./{path}/Embeddings\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            np.save(f\"./{path}/Embeddings/{i}.npy\", embeddings)\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(file_paths):\n",
    "#     get_embeddings(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Parallel(n_jobs=3, backend='loky')(delayed(get_embeddings)(i) for i in tqdm(file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naweenk/miniforge3/envs/demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", run_opts={\"device\":\"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.16'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speechbrain\n",
    "speechbrain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal shape:  torch.Size([2, 480384])\n",
    "shape:,  torch.Size([2, 1, 192])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal shape:  torch.Size([1, 480384])\n",
      "signal shape:  torch.Size([2, 480384])\n",
      "shape:,  torch.Size([2, 1, 192])\n"
     ]
    }
   ],
   "source": [
    "signal, fs = torchaudio.load(\"2.wav\")\n",
    "print(\"signal shape: \", signal.shape)\n",
    "# print(\"scale\")\n",
    "# signal = scale_signals(683433, signal)\n",
    "# print(\"encode\")\n",
    "if signal.shape[0] == 1:\n",
    "    signal = signal.repeat(2, 1)\n",
    "print(\"signal shape: \", signal.shape)\n",
    "embedding = classifier.encode_batch(signal)\n",
    "print(\"shape:, \", embedding.shape)\n",
    "# print(\"to numpy\")\n",
    "# embedding = signal\n",
    "embedding = embedding.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "head",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
